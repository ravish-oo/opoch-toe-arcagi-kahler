# WO-01 — ARC Loader (Π inputs)

## Purpose (what this unit does)

Load a single ARC task JSON from disk, validate it against the canonical ARC format, and emit a **canonicalized in-memory object** ready for Π, invariants, and solver stages. Format rules are frozen from the official ARC sources: tasks are JSON dicts with keys `"train"` and `"test"`, each item is a “pair” with `"input"` and `"output"` (test has no output), and each grid is a rectangular matrix of integers 0–9; typical train count is ~3 pairs, test is 1 pair. ([GitHub][1])

## Anchor docs to read first

* `docs/anchors/00-vision-universe.md` — A0–A2, “two moves,” Π/GLUE/FY guardrails.
* `docs/anchors/01-arc-on-the-cloth.md` — ARC↔cloth mapping (grids, Π, Z, G_free).
* `docs/anchors/02-impl-contracts.md` — fixed API signatures & shapes for loader/canonicalizer.

Stay within these; no widening of scope.

---

## Scope (v1, nothing more)

* Parse a task JSON file from filesystem.
* Validate shape and values against ARC rules (rectangular grids, 0–9 entries, sizes 1..30). ([Kaggle][2])
* Normalize palette to contiguous `[0..C-1]` within each task (do **not** change geometry here; D4 comes in the next WO).
* Return a **typed dict** with `train` and `test`, preserving original integer grids for later canonicalization.

**Out of scope (v1):** no D4 pose changes, no resizing, no heuristics. That’s WO-02.

---

## Dependencies (pin exact libs to avoid wheel-reinventing)

* Python stdlib only: `json`, `pathlib`, `typing`, `dataclasses`.
* Optional: `numpy` for fast shape checks; not required.

No third-party parser. The ARC JSON format is simple and documented. ([GitHub][1])

---

## Public API (exact signatures)

`arc_cloth/io/arc_loader.py`

```python
from __future__ import annotations
from typing import TypedDict, List, Dict, Any
from pathlib import Path

class Grid = List[List[int]]

class TrainPair(TypedDict):
    input: Grid
    output: Grid

class TestPair(TypedDict):
    input: Grid

class Task(TypedDict):
    train: List[TrainPair]
    test:  List[TestPair]

def load_task(path: str | Path) -> Task:
    """
    Load & validate an ARC task JSON.

    Validations (hard failures with descriptive exceptions):
      - JSON root has exactly keys {"train","test"}.
      - train is a non-empty list of pairs with keys {"input","output"}.
      - test is a list of exactly one pair with key {"input"}.
      - Each grid is a rectangular HxW matrix (1<=H,W<=30), ints in [0..9].
      - All train inputs/outputs and the test input for a task share a palette subset of [0..9].
    Returns a normalized Task dict with ints and Python lists.
    Emits receipts via return metadata (see below).
    """
```

**Return metadata** (attach as attributes or a sibling dict `__meta__` that travels with the Task):

* `meta.shape_summary`: list of `(H,W)` per grid.
* `meta.palette`: sorted unique colors for this task.
* `meta.receipts`: Π-format receipts (see “Receipts to emit”).

These fields are **for debugging** and the reviewer; they don’t alter the solver.

---

## Implementation details (explicit, so Claude doesn’t improvise)

1. **Read/parse**

   * Use `json.loads(Path(path).read_text(encoding="utf-8"))`.
   * Disallow trailing fields or missing keys; raise `ValueError` with the exact missing/extra key.

2. **Validate structure** against ARC format (hard errors):

   * `root.keys()=={"train","test"}`; train is list of ≥1 pairs; test is list of **exactly one** pair (ARC/ARC-AGI-2 occasionally has 1–2 tests; we fix v1 at 1 for consistency — if 2 found, raise `NotImplementedError` with message “v1 supports a single test input”). ([GitHub][1])
   * Each pair is dict with `input` (and `output` for train).

3. **Validate grids**

   * Rectangular: all rows same length; sizes 1..30; entries are ints 0..9. ([Kaggle][2])
   * If any non-int or out-of-range value, raise with `(i,j,value)` context.

4. **Palette normalization (within task)**

   * Compute sorted unique colors across all train inputs/outputs and test input.
   * Map them to contiguous `[0..C-1]` by stable order of first appearance (record mapping in `meta.palette_map`), and rewrite all grids accordingly.
   * This is Π-safe (labels only); geometry untouched per 01-doc.

5. **Shape summary**

   * Collect `(H,W)` for each grid; put in `meta.shape_summary`.

6. **No file system globbing** here. Batch runs belong in the runner and reviewer script.

---

## Receipts to emit (first-class, machine-checkable)

Emit a `meta.receipts` dict with:

* **Π-format-idempotence**: Re-serialize the validated object and parse it again inside this function; assert equality (`==`) with the already normalized object. If not equal, raise; this proves loader normalization is idempotent under our rules (labels only).
* **Spec compliance**: Booleans for `has_train`, `has_single_test`, `rectangular`, `values_in_0_9`, `sizes_in_1_30`.
* **Palette proof**: original palette set and remapped palette `[0..C-1]`.

These receipts land in the review logs to make failures obvious.

Grounding for the above spec checks comes from the ARC repo & prize guide (JSON with `train`/`test`, grid integers 0–9, typical 3 trains, one test, rectangular grids). ([GitHub][1])

---

## Debugging playbook (implementer)

* **Malformed JSON**: print the exact offset; re-dump a minimal repro snippet.
* **Key mismatch**: show `root.keys()` vs expected.
* **Grid non-rectangular**: show row lengths and first offending row index.
* **Out-of-range value**: show `(i,j,value)` and grid name `train[k].input|output` or `test[0].input`.
* **Palette map check**: log the mapping dict `{old_color: new_index}` once per task.

All exceptions must be **actionable**: message includes path and offending field.

---

## Reviewer + tester (how to run, what to look for)

**Before running**: Skim `00-vision-universe.md` §1–3 (Π and “negation first”), and `01-arc-on-the-cloth.md` §1–2 (data model; Z and Π). This WO only touches Π inputs.

**Batch script (temporary, simple CPU)**
Write a one-off script `scripts/validate_all.py` that:

```python
from pathlib import Path
from arc_cloth.io.arc_loader import load_task

root = Path("PATH_TO_ARC_JSON_DIR")  # e.g., ARC-AGI repo tasks dir
errors = []
for p in sorted(root.rglob("*.json")):
    try:
        task = load_task(p)
    except Exception as e:
        errors.append((p, str(e)))
print(f"checked {len(list(root.rglob('*.json')))} files; failures={len(errors)}")
for p,msg in errors[:50]:
    print(p, "->", msg)
```

Run it over **all 1000 ARC-AGI train tasks and the evaluation sets**; format and grid rules are consistent across the corpus. Kaggle/ARC pages confirm corpus size & JSON structure expectations. ([Kaggle][3])

**Receipts to verify per file**

* `meta.receipts["spec_compliance"]` booleans are all `True`.
* `meta.receipts["pi_idempotent"]` is `True`.
* `meta.palette` equals sorted unique colors; `meta.palette_map` is contiguous `[0..C-1]`.

**Marking unsatisfiable tasks (loader-level)**
If a file violates ARC format (very rare), it’s unsatisfiable *at WO-01 stage* and should be recorded in `scripts/validate_all.py` output. Otherwise, do **not** classify “hard” tasks here; that belongs to solver WOs.

---

## Acceptance criteria (for both implementer and reviewer)

* **A. Spec-faithful**: JSON layout `train/test` and grid constraints enforced exactly as ARC sources describe. ([GitHub][1])
* **B. Deterministic**: Running `load_task` twice returns identical objects (including remapped palettes).
* **C. Π-Ready**: No geometry changes; only palette relabeling to contiguous `[0..C-1]`.
* **D. Receipts OK**: All receipt flags present and true on valid tasks; failures are actionable.
* **E. Corpus-wide**: Batch validator completes across the full ARC directory with a clear list of any format failures.

---

[1]: https://github.com/fchollet/ARC-AGI?utm_source=chatgpt.com "fchollet/ARC-AGI: The Abstraction and Reasoning Corpus"
[2]: https://www.kaggle.com/code/allegich/arc-agi-2025-starter-notebook-eda?utm_source=chatgpt.com "ARC-AGI 2025: Starter notebook + EDA"
[3]: https://www.kaggle.com/code/rahuljaisy/arc-agi-multi-disciplinary-eda-exploration?utm_source=chatgpt.com "ARC-AGI: Multi-Disciplinary EDA Exploration"
